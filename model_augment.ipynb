{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import torch\n",
    "import random\n",
    "import timeit\n",
    "import torchaudio\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from scipy import signal\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import recall_score\n",
    "from torch.utils.data import TensorDataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot waveform\n",
    "\n",
    "def plot_waveform(waveform, sample_rate, title=\"Waveform\", xlim=None, ylim=None):\n",
    "    waveform = waveform.numpy()\n",
    "\n",
    "    num_channels, num_frames = waveform.shape\n",
    "    time_axis = torch.arange(0, num_frames) / sample_rate\n",
    "\n",
    "    figure, axes = plt.subplots(num_channels, 1)\n",
    "    if num_channels == 1:\n",
    "        axes = [axes]\n",
    "    for c in range(num_channels):\n",
    "        axes[c].plot(time_axis, waveform[c], linewidth=1)\n",
    "        axes[c].grid(True)\n",
    "        if num_channels > 1:\n",
    "            axes[c].set_ylabel(f'Channel {c+1}')\n",
    "        if xlim:\n",
    "            axes[c].set_xlim(xlim)\n",
    "        if ylim:\n",
    "            axes[c].set_ylim(ylim)\n",
    "    figure.suptitle(title)\n",
    "    plt.show(block=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_specgram(waveform, sample_rate, title=\"Spectrogram\", xlim=None):\n",
    "    waveform = waveform.numpy()\n",
    "\n",
    "    num_channels, num_frames = waveform.shape\n",
    "    time_axis = torch.arange(0, num_frames) / sample_rate\n",
    "\n",
    "    figure, axes = plt.subplots(num_channels, 1)\n",
    "    if num_channels == 1:\n",
    "        axes = [axes]\n",
    "    for c in range(num_channels):\n",
    "        axes[c].specgram(waveform[c], Fs=sample_rate)\n",
    "        if num_channels > 1:\n",
    "            axes[c].set_ylabel(f'Channel {c+1}')\n",
    "        if xlim:\n",
    "            axes[c].set_xlim(xlim)\n",
    "    figure.suptitle(title)\n",
    "    plt.show(block=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data preparation\n",
    "path = \"/Users/liyuanchao/Documents/Corpora/SSPNet/SSPNet/Audio\"\n",
    "os.chdir(path)\n",
    "\n",
    "train_ex = []\n",
    "test_ex= []\n",
    "train_ag = []\n",
    "test_ag= []\n",
    "train_co = []\n",
    "test_co= []\n",
    "train_ne = []\n",
    "test_ne= []\n",
    "train_op = []\n",
    "test_op= []\n",
    "feats_train = []\n",
    "feats_test = []\n",
    "\n",
    "spectrogram = torchaudio.transforms.Spectrogram()\n",
    "fmask1 = torchaudio.transforms.FrequencyMasking(freq_mask_param=20)\n",
    "fmask2 = torchaudio.transforms.FrequencyMasking(freq_mask_param=100)\n",
    "tmask1 = torchaudio.transforms.TimeMasking(time_mask_param=20)\n",
    "tmask2 = torchaudio.transforms.TimeMasking(time_mask_param=100)\n",
    "\n",
    "with open('Scores.csv', 'r') as f:\n",
    "    file_content = csv.reader(f, delimiter=',')\n",
    "    headers = next(file_content, None)\n",
    "    \n",
    "    for row in list(file_content)[:512]:\n",
    "        waveform, sr = torchaudio.load(row[0] + '.wav', normalize=True)\n",
    "        waveform = torch.squeeze(waveform, 0)\n",
    "        waveform = waveform[1:]\n",
    "        if len(waveform) < 80000:\n",
    "            waveform = waveform.numpy()\n",
    "            waveform = np.pad(waveform, (0, 80000-len(waveform)), 'constant', constant_values=0)\n",
    "            waveform = torch.from_numpy(waveform)\n",
    "       \n",
    "        original = spectrogram(waveform)\n",
    "        \n",
    "        fmasked1 = fmask1(original)\n",
    "        fmasked2 = fmask2(original)\n",
    "        tmasked1 = tmask1(original)\n",
    "        tmasked2 = tmask2(original)\n",
    "                \n",
    "        feats_train.append(original.numpy())\n",
    "        train_ex.append(int(row[1]))\n",
    "        train_ag.append(int(row[2]))\n",
    "        train_co.append(int(row[3]))\n",
    "        train_ne.append(int(row[4]))\n",
    "        train_op.append(int(row[5]))\n",
    "        \n",
    "        feats_train.append(fmasked1.numpy())\n",
    "        train_ex.append(int(row[1]))\n",
    "        train_ag.append(int(row[2]))\n",
    "        train_co.append(int(row[3]))\n",
    "        train_ne.append(int(row[4]))\n",
    "        train_op.append(int(row[5]))\n",
    "        \n",
    "        feats_train.append(fmasked2.numpy())\n",
    "        train_ex.append(int(row[1]))\n",
    "        train_ag.append(int(row[2]))\n",
    "        train_co.append(int(row[3]))\n",
    "        train_ne.append(int(row[4]))\n",
    "        train_op.append(int(row[5]))\n",
    "        \n",
    "        feats_train.append(tmasked1.numpy())\n",
    "        train_ex.append(int(row[1]))\n",
    "        train_ag.append(int(row[2]))\n",
    "        train_co.append(int(row[3]))\n",
    "        train_ne.append(int(row[4]))\n",
    "        train_op.append(int(row[5]))\n",
    "        \n",
    "        feats_train.append(tmasked2.numpy())\n",
    "        train_ex.append(int(row[1]))\n",
    "        train_ag.append(int(row[2]))\n",
    "        train_co.append(int(row[3]))\n",
    "        train_ne.append(int(row[4]))\n",
    "        train_op.append(int(row[5]))\n",
    "\n",
    "f.close()\n",
    "        \n",
    "with open('Scores.csv', 'r') as f:\n",
    "    file_content = csv.reader(f, delimiter=',')\n",
    "    headers = next(file_content, None)\n",
    "    \n",
    "    for row in list(file_content)[512:]:\n",
    "        waveform, sr = torchaudio.load(row[0] + '.wav', normalize=True)\n",
    "        waveform = torch.squeeze(waveform, 0)\n",
    "        waveform = waveform[1:]\n",
    "        if len(waveform) < 80000:\n",
    "            waveform = waveform.numpy()\n",
    "            waveform = np.pad(waveform, (0, 80000-len(waveform)), 'constant', constant_values=0)\n",
    "            waveform = torch.from_numpy(waveform)\n",
    "       \n",
    "        original = spectrogram(waveform)\n",
    "                \n",
    "        feats_test.append(original.numpy())\n",
    "        test_ex.append(int(row[1]))\n",
    "        test_ag.append(int(row[2]))\n",
    "        test_co.append(int(row[3]))\n",
    "        test_ne.append(int(row[4]))\n",
    "        test_op.append(int(row[5]))\n",
    "\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "feats_test = feats_train[int(0.8*3200):]\n",
    "feats_train = feats_train[:int(0.8*3200)]\n",
    "test_ex = train_ex[int(0.8*3200):]\n",
    "test_ag = train_ag[int(0.8*3200):]\n",
    "test_co = train_co[int(0.8*3200):]\n",
    "test_ne = train_ne[int(0.8*3200):]\n",
    "test_op = train_op[int(0.8*3200):]\n",
    "train_ex = train_ex[:int(0.8*3200)]\n",
    "train_ag = train_ag[:int(0.8*3200)]\n",
    "train_co = train_co[:int(0.8*3200)]\n",
    "train_ne = train_ne[:int(0.8*3200)]\n",
    "train_op = train_op[:int(0.8*3200)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "feats_train = torch.tensor(feats_train)\n",
    "feats_test = torch.tensor(feats_test)\n",
    "train_ex = torch.tensor(train_ex)\n",
    "train_ag = torch.tensor(train_ag)\n",
    "train_co = torch.tensor(train_co)\n",
    "train_ne = torch.tensor(train_ne)\n",
    "train_op = torch.tensor(train_op)\n",
    "test_ex = torch.tensor(test_ex)\n",
    "test_ag = torch.tensor(test_ag)\n",
    "test_co = torch.tensor(test_co)\n",
    "test_ne = torch.tensor(test_ne)\n",
    "test_op = torch.tensor(test_op)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalization\n",
    "feats_train = F.normalize(feats_train, dim=0)\n",
    "feats_test = F.normalize(feats_test, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2560, 201, 401]) torch.Size([640, 201, 401])\n"
     ]
    }
   ],
   "source": [
    "print(feats_train.size(), feats_test.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data preparation completed!\n"
     ]
    }
   ],
   "source": [
    "trainset = TensorDataset(feats_train, train_ex, train_ag, train_co, train_ne, train_op)\n",
    "testset = TensorDataset(feats_test, test_ex, test_ag, test_co, test_ne, test_op)\n",
    "\n",
    "traindata = DataLoader(dataset=trainset, batch_size=64, shuffle=False)\n",
    "testdata = DataLoader(dataset=testset, batch_size=64, shuffle=False)\n",
    "\n",
    "print('Data preparation completed!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model augmentation\n",
    "class NeuralNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNet, self).__init__()\n",
    "        self.cnn = nn.Conv1d(401, 100, kernel_size=3, stride=2)\n",
    "        self.pool = nn.MaxPool1d(2)\n",
    "        self.lstm = nn.LSTM(input_size=100,\n",
    "                            hidden_size=64,\n",
    "                            num_layers=2,\n",
    "                            dropout=0.5,\n",
    "                            batch_first=True,\n",
    "                            bidirectional=True)\n",
    "        self.drop = nn.Dropout(p=0.5)\n",
    "        self.attn = nn.MultiheadAttention(128, 16, batch_first=True)\n",
    "        self.flat = nn.Flatten()\n",
    "        self.dense = nn.Linear(10000, 512)\n",
    "        self.dense2 = nn.Linear(512, 32)\n",
    "        self.acti = nn.ReLU()\n",
    "        self.out1 = nn.Linear(32, 2)\n",
    "        self.out2 = nn.Linear(32, 2)\n",
    "        self.out3 = nn.Linear(32, 2)\n",
    "        self.out4 = nn.Linear(32, 2)\n",
    "        self.out5 = nn.Linear(32, 2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # cnn\n",
    "        x = x.transpose(1,2)\n",
    "        x = self.cnn(x)\n",
    "        x = x.transpose(1,2)\n",
    "#         x = self.pool(x)\n",
    "        # lstm\n",
    "#         self.lstm.flatten_parameters()\n",
    "#         x, _ = self.lstm(x)\n",
    "#         x = self.drop(x)\n",
    "        # attention\n",
    "#         x, _ = self.attn(x,x,x)\n",
    "        # dense\n",
    "        x = self.flat(x)\n",
    "        x = self.drop(x)\n",
    "        x = self.dense(x)\n",
    "        x = self.drop(x)\n",
    "        x = self.dense2(x)\n",
    "        x = self.drop(x)\n",
    "        x = self.acti(x)\n",
    "        # out\n",
    "#         ex = self.out1(x)\n",
    "        ag = self.out2(x)\n",
    "#         co = self.out3(x)\n",
    "#         ne = self.out4(x)\n",
    "#         op = self.out5(x)\n",
    "        return ag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----epoch:  0 -----\n",
      "--training begins--\n",
      "--test begins--\n",
      "train_loss: 0.6943 |test_loss: 0.6960 \n",
      "train_acc_ex: 0.4774 test_acc_ex: 0.5000\n",
      "Time:  7.308648375000075\n",
      "-----epoch:  1 -----\n",
      "--training begins--\n",
      "--test begins--\n",
      "train_loss: 0.6910 |test_loss: 0.6957 \n",
      "train_acc_ex: 0.5000 test_acc_ex: 0.5000\n",
      "Time:  7.176836500000036\n",
      "-----epoch:  2 -----\n",
      "--training begins--\n",
      "--test begins--\n",
      "train_loss: 0.6860 |test_loss: 0.6967 \n",
      "train_acc_ex: 0.5201 test_acc_ex: 0.5177\n",
      "Time:  7.213723084000094\n",
      "-----epoch:  3 -----\n",
      "--training begins--\n",
      "--test begins--\n",
      "train_loss: 0.6773 |test_loss: 0.7000 \n",
      "train_acc_ex: 0.5710 test_acc_ex: 0.5022\n",
      "Time:  7.237470708000046\n",
      "-----epoch:  4 -----\n",
      "--training begins--\n",
      "--test begins--\n",
      "train_loss: 0.6570 |test_loss: 0.7011 \n",
      "train_acc_ex: 0.6250 test_acc_ex: 0.4941\n",
      "Time:  7.242279000000053\n",
      "-----epoch:  5 -----\n",
      "--training begins--\n",
      "--test begins--\n",
      "train_loss: 0.6536 |test_loss: 0.7027 \n",
      "train_acc_ex: 0.6428 test_acc_ex: 0.4818\n",
      "Time:  7.488836291000098\n",
      "-----epoch:  6 -----\n",
      "--training begins--\n",
      "--test begins--\n",
      "train_loss: 0.6489 |test_loss: 0.7048 \n",
      "train_acc_ex: 0.6552 test_acc_ex: 0.4800\n",
      "Time:  7.484304042000076\n",
      "-----epoch:  7 -----\n",
      "--training begins--\n",
      "--test begins--\n",
      "train_loss: 0.6483 |test_loss: 0.7067 \n",
      "train_acc_ex: 0.6561 test_acc_ex: 0.4860\n",
      "Time:  7.240630874999852\n"
     ]
    }
   ],
   "source": [
    "#augmentation\n",
    "\n",
    "torch.manual_seed(1)\n",
    "model = NeuralNet()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=4, gamma=0.1)\n",
    "func = nn.CrossEntropyLoss()\n",
    "# func = nn.BCEWithLogitsLoss(weight=None, size_average=None, reduce=None, reduction='mean', pos_weight=None)\n",
    "\n",
    "# training\n",
    "for epoch in range(8):\n",
    "    start = timeit.default_timer()\n",
    "    print(\"-----epoch: \", epoch, \"-----\")\n",
    "    loss_list_train_ex = []\n",
    "    loss_list_train_ag = []\n",
    "    loss_list_train_co = []\n",
    "    loss_list_train_ne = []\n",
    "    loss_list_train_op = []\n",
    "    loss_list_test_ex = []\n",
    "    loss_list_test_ag = []\n",
    "    loss_list_test_co = []\n",
    "    loss_list_test_ne = []\n",
    "    loss_list_test_op = []\n",
    "    \n",
    "    pred_train_ex = []\n",
    "    pred_train_ag = []\n",
    "    pred_train_co = []\n",
    "    pred_train_ne = []\n",
    "    pred_train_op = []\n",
    "    pred_test_ex = []\n",
    "    pred_test_ag = []\n",
    "    pred_test_co = []\n",
    "    pred_test_ne = []\n",
    "    pred_test_op = []\n",
    "    \n",
    "    label_train_ex = []\n",
    "    label_train_ag = []\n",
    "    label_train_co = []\n",
    "    label_train_ne = []\n",
    "    label_train_op = []\n",
    "    \n",
    "    print('--training begins--')\n",
    "    model.train()\n",
    "    for feats_input, label_ex, label_ag, label_co, label_ne, label_op in traindata:\n",
    "        # loss\n",
    "#         pred_ex, pred_ag, pred_co, pred_ne, pred_op = model(feats_input)\n",
    "        pred_ag = model(feats_input)\n",
    "            \n",
    "#         train_loss_ex = func(pred_ex, label_ex)\n",
    "        train_loss_ag = func(pred_ag, label_ag)\n",
    "#         train_loss_co = func(pred_co, label_co)\n",
    "#         train_loss_ne = func(pred_ne, label_ne)\n",
    "#         train_loss_op = func(pred_op, label_op)\n",
    "        \n",
    "#         loss_list_train_ex.append(train_loss_ex.item())\n",
    "        loss_list_train_ag.append(train_loss_ag.item())\n",
    "#         loss_list_train_co.append(train_loss_co.item())\n",
    "#         loss_list_train_ne.append(train_loss_ne.item())\n",
    "#         loss_list_train_op.append(train_loss_op.item())\n",
    "        \n",
    "#         for i in label_ex:\n",
    "#             label_train_ex.append(i.detach().numpy())\n",
    "        for i in label_ag:\n",
    "            label_train_ag.append(i.detach().numpy())\n",
    "#         for i in label_co:\n",
    "#             label_train_co.append(i.detach().numpy())\n",
    "#         for i in label_ne:\n",
    "#             label_train_ne.append(i.detach().numpy())\n",
    "#         for i in label_op:\n",
    "#             label_train_op.append(i.detach().numpy())\n",
    "\n",
    "# #         for i in pred_ex:\n",
    "# #             pred_train_ex.append(i.detach().numpy())\n",
    "        for i in pred_ag:\n",
    "            pred_train_ag.append(i.detach().numpy())\n",
    "#         for i in pred_co:\n",
    "#             pred_train_co.append(i.detach().numpy())\n",
    "#         for i in pred_ne:\n",
    "#             pred_train_ne.append(i.detach().numpy())\n",
    "#         for i in pred_op:\n",
    "#             pred_train_op.append(i.detach().numpy())\n",
    "            \n",
    "        # backprop\n",
    "        optimizer.zero_grad()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "#         train_loss = train_loss_ex + train_loss_ag + train_loss_co + train_loss_ne + train_loss_op\n",
    "        train_loss = train_loss_ag\n",
    "        train_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "# test\n",
    "    print('--test begins--')\n",
    "    model.eval()\n",
    "    for feats_input, label_ex, label_ag, label_co, label_ne, label_op in testdata:\n",
    "        # loss\n",
    "#         pred_ex, pred_ag, pred_co, pred_ne, pred_op = model(feats_input)\n",
    "        pred_ag = model(feats_input)\n",
    "        \n",
    "#         test_loss_ex = func(pred_ex, label_ex)\n",
    "        test_loss_ag = func(pred_ag, label_ag)\n",
    "#         test_loss_co = func(pred_co, label_co)\n",
    "#         test_loss_ne = func(pred_ne, label_ne)\n",
    "#         test_loss_op = func(pred_op, label_op)\n",
    "\n",
    "#         loss_list_test_ex.append(test_loss_ex.item())\n",
    "        loss_list_test_ag.append(test_loss_ag.item())\n",
    "#         loss_list_test_co.append(test_loss_co.item())\n",
    "#         loss_list_test_ne.append(test_loss_ne.item())\n",
    "#         loss_list_test_op.append(test_loss_op.item())\n",
    "\n",
    "#         for i in pred_ex:\n",
    "#             pred_test_ex.append(i.detach().numpy())\n",
    "        for i in pred_ag:\n",
    "            pred_test_ag.append(i.detach().numpy())\n",
    "#         for i in pred_co:\n",
    "#             pred_test_co.append(i.detach().numpy())\n",
    "#         for i in pred_ne:\n",
    "#             pred_test_ne.append(i.detach().numpy())\n",
    "#         for i in pred_op:\n",
    "#             pred_test_op.append(i.detach().numpy())\n",
    "\n",
    "    # accuracy\n",
    "#     label_train_ex = np.array(label_train_ex)\n",
    "    label_train_ag = np.array(label_train_ag)\n",
    "#     label_train_co = np.array(label_train_co)\n",
    "#     label_train_ne = np.array(label_train_ne)\n",
    "#     label_train_op = np.array(label_train_op)\n",
    "#     pred_train_ex = np.array(pred_train_ex)\n",
    "    pred_train_ag = np.array(pred_train_ag)\n",
    "#     pred_train_co = np.array(pred_train_co)\n",
    "#     pred_train_ne = np.array(pred_train_ne)\n",
    "#     pred_train_op = np.array(pred_train_op)\n",
    "#     pred_test_ex = np.array(pred_test_ex)\n",
    "    pred_test_ag = np.array(pred_test_ag)\n",
    "#     pred_test_co = np.array(pred_test_co)\n",
    "#     pred_test_ne = np.array(pred_test_ne)\n",
    "#     pred_test_op = np.array(pred_test_op)\n",
    "\n",
    "#     pred_train_ex = [np.argmax(p) for p in pred_train_ex]\n",
    "    pred_train_ag = [np.argmax(p) for p in pred_train_ag]\n",
    "#     pred_train_co = [np.argmax(p) for p in pred_train_co]\n",
    "#     pred_train_ne = [np.argmax(p) for p in pred_train_ne]\n",
    "#     pred_train_op = [np.argmax(p) for p in pred_train_op]\n",
    "#     pred_test_ex = [np.argmax(p) for p in pred_test_ex]\n",
    "    pred_test_ag = [np.argmax(p) for p in pred_test_ag]\n",
    "#     pred_test_co = [np.argmax(p) for p in pred_test_co]\n",
    "#     pred_test_ne = [np.argmax(p) for p in pred_test_ne]\n",
    "#     pred_test_op = [np.argmax(p) for p in pred_test_op]\n",
    "\n",
    "#     print(np.rint(pred_train_ex))\n",
    "\n",
    "#     train_acc_ex = recall_score(label_train_ex, pred_train_ex, average='macro')\n",
    "    train_acc_ag = recall_score(label_train_ag, np.rint(pred_train_ag), average='macro')\n",
    "#     train_acc_co = recall_score(label_train_co, np.rint(pred_train_co), average='macro')\n",
    "#     train_acc_ne = recall_score(label_train_ne, np.rint(pred_train_ne), average='macro')\n",
    "#     train_acc_op = recall_score(label_train_op, np.rint(pred_train_op), average='macro')\n",
    "#     test_acc_ex = recall_score(test_ex.numpy(), pred_test_ex, average='macro')\n",
    "    test_acc_ag = recall_score(test_ag.numpy(), np.rint(pred_test_ag), average='macro')\n",
    "#     test_acc_co = recall_score(test_co.numpy(), np.rint(pred_test_co), average='macro')\n",
    "#     test_acc_ne = recall_score(test_ne.numpy(), np.rint(pred_test_ne), average='macro')\n",
    "#     test_acc_op = recall_score(test_op.numpy(), np.rint(pred_test_op), average='macro')\n",
    "\n",
    "    train_loss = sum(loss_list_train_ag) / len(loss_list_train_ag)\n",
    "\n",
    "\n",
    "#     (sum(loss_list_train_ex) / len(loss_list_train_ex)) \n",
    "#     sum(loss_list_train_ag) / len(loss_list_train_ag)\n",
    "#     + (sum(loss_list_train_co) / len(loss_list_train_co))\n",
    "#     + (sum(loss_list_train_ne) / len(loss_list_train_ne))\n",
    "#     + (sum(loss_list_train_op) / len(loss_list_train_op))\n",
    "    test_loss = sum(loss_list_test_ag) / len(loss_list_test_ag)\n",
    "\n",
    "\n",
    "#     (sum(loss_list_test_ex) / len(loss_list_test_ex))\n",
    "#     (sum(loss_list_test_ag) / len(loss_list_test_ag)\n",
    "#     + (sum(loss_list_test_co) / len(loss_list_test_co))\n",
    "#     + (sum(loss_list_test_ne) / len(loss_list_test_ne))\n",
    "#     + (sum(loss_list_test_op) / len(loss_list_test_op))\n",
    "        \n",
    "#     print('train_loss: %.4f' % train_loss, '|test_loss: %.4f' % test_loss, '\\n'\n",
    "#           'train_acc_ex: %.4f' % train_acc_ex, '|train_acc_ag: %.4f' % train_acc_ag, '|train_acc_co: %.4f' % train_acc_co, '|train_acc_ne: %.4f' % train_acc_ne, '|train_acc_op: %.4f' % train_acc_op, '\\n'\n",
    "#           'test_acc_ex: %.4f' % test_acc_ex, '|test_acc_ag: %.4f' % test_acc_ag, '|test_acc_co: %.4f' % test_acc_co, '|test_acc_ne: %.4f' % test_acc_ne, '|test_acc_op: %.4f' % test_acc_op)\n",
    "\n",
    "    print('train_loss: %.4f' % train_loss, '|test_loss: %.4f' % test_loss, '\\n'\n",
    "          'train_acc_ex: %.4f' % train_acc_ag, 'test_acc_ex: %.4f' % test_acc_ag)\n",
    "\n",
    "    stop = timeit.default_timer()\n",
    "    print('Time: ', stop - start)\n",
    "    scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----epoch:  0 -----\n",
      "--training begins--\n",
      "--test begins--\n",
      "train_loss: 0.5764 |test_loss: 0.6983 \n",
      "train_acc_ex: 0.7085 test_acc_ex: 0.5641\n",
      "Time:  7.2711489579999125\n",
      "-----epoch:  1 -----\n",
      "--training begins--\n",
      "--test begins--\n",
      "train_loss: 0.5741 |test_loss: 0.6987 \n",
      "train_acc_ex: 0.7033 test_acc_ex: 0.5641\n",
      "Time:  7.261592084000085\n",
      "-----epoch:  2 -----\n",
      "--training begins--\n",
      "--test begins--\n",
      "train_loss: 0.5748 |test_loss: 0.6990 \n",
      "train_acc_ex: 0.7047 test_acc_ex: 0.5641\n",
      "Time:  7.292436541999905\n",
      "-----epoch:  3 -----\n",
      "--training begins--\n",
      "--test begins--\n",
      "train_loss: 0.5735 |test_loss: 0.6993 \n",
      "train_acc_ex: 0.7049 test_acc_ex: 0.5641\n",
      "Time:  7.289051208000046\n",
      "-----epoch:  4 -----\n",
      "--training begins--\n",
      "--test begins--\n",
      "train_loss: 0.5721 |test_loss: 0.6997 \n",
      "train_acc_ex: 0.7124 test_acc_ex: 0.5641\n",
      "Time:  7.246960959000035\n",
      "-----epoch:  5 -----\n",
      "--training begins--\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-c4612d00be44>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[0;31m#         train_loss = train_loss_ex + train_loss_ag + train_loss_co + train_loss_ne + train_loss_op\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m         \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_loss_ex\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m         \u001b[0mtrain_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    253\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 255\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    256\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    147\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m    148\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 149\u001b[0;31m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#augmentation\n",
    "\n",
    "# torch.manual_seed(1)\n",
    "# model = NeuralNet()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-6)\n",
    "# scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=4, gamma=0.1)\n",
    "# func = nn.CrossEntropyLoss()\n",
    "# func = nn.BCEWithLogitsLoss(weight=None, size_average=None, reduce=None, reduction='mean', pos_weight=None)\n",
    "\n",
    "# training\n",
    "for epoch in range(32):\n",
    "    start = timeit.default_timer()\n",
    "    print(\"-----epoch: \", epoch, \"-----\")\n",
    "    loss_list_train_ex = []\n",
    "    loss_list_train_ag = []\n",
    "    loss_list_train_co = []\n",
    "    loss_list_train_ne = []\n",
    "    loss_list_train_op = []\n",
    "    loss_list_test_ex = []\n",
    "    loss_list_test_ag = []\n",
    "    loss_list_test_co = []\n",
    "    loss_list_test_ne = []\n",
    "    loss_list_test_op = []\n",
    "    \n",
    "    pred_train_ex = []\n",
    "    pred_train_ag = []\n",
    "    pred_train_co = []\n",
    "    pred_train_ne = []\n",
    "    pred_train_op = []\n",
    "    pred_test_ex = []\n",
    "    pred_test_ag = []\n",
    "    pred_test_co = []\n",
    "    pred_test_ne = []\n",
    "    pred_test_op = []\n",
    "    \n",
    "    label_train_ex = []\n",
    "    label_train_ag = []\n",
    "    label_train_co = []\n",
    "    label_train_ne = []\n",
    "    label_train_op = []\n",
    "    \n",
    "    print('--training begins--')\n",
    "    model.train()\n",
    "    for feats_input, label_ex, label_ag, label_co, label_ne, label_op in traindata:\n",
    "        # loss\n",
    "        pred_ex, pred_ag, pred_co, pred_ne, pred_op = model(feats_input)\n",
    "#         pred_ex = model(feats_input)\n",
    "            \n",
    "        train_loss_ex = func(pred_ex, label_ex)\n",
    "        train_loss_ag = func(pred_ag, label_ag)\n",
    "        train_loss_co = func(pred_co, label_co)\n",
    "        train_loss_ne = func(pred_ne, label_ne)\n",
    "        train_loss_op = func(pred_op, label_op)\n",
    "        \n",
    "        loss_list_train_ex.append(train_loss_ex.item())\n",
    "        loss_list_train_ag.append(train_loss_ag.item())\n",
    "        loss_list_train_co.append(train_loss_co.item())\n",
    "        loss_list_train_ne.append(train_loss_ne.item())\n",
    "        loss_list_train_op.append(train_loss_op.item())\n",
    "        \n",
    "        for i in label_ex:\n",
    "            label_train_ex.append(i.detach().numpy())\n",
    "        for i in label_ag:\n",
    "            label_train_ag.append(i.detach().numpy())\n",
    "        for i in label_co:\n",
    "            label_train_co.append(i.detach().numpy())\n",
    "        for i in label_ne:\n",
    "            label_train_ne.append(i.detach().numpy())\n",
    "        for i in label_op:\n",
    "            label_train_op.append(i.detach().numpy())\n",
    "\n",
    "        for i in pred_ex:\n",
    "            pred_train_ex.append(i.detach().numpy())\n",
    "        for i in pred_ag:\n",
    "            pred_train_ag.append(i.detach().numpy())\n",
    "        for i in pred_co:\n",
    "            pred_train_co.append(i.detach().numpy())\n",
    "        for i in pred_ne:\n",
    "            pred_train_ne.append(i.detach().numpy())\n",
    "        for i in pred_op:\n",
    "            pred_train_op.append(i.detach().numpy())\n",
    "            \n",
    "        # backprop\n",
    "        optimizer.zero_grad()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        train_loss = train_loss_ex + train_loss_ag + train_loss_co + train_loss_ne + train_loss_op\n",
    "#         train_loss = train_loss_ex\n",
    "        train_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "# test\n",
    "    print('--test begins--')\n",
    "    model.eval()\n",
    "    for feats_input, label_ex, label_ag, label_co, label_ne, label_op in testdata:\n",
    "        # loss\n",
    "        pred_ex, pred_ag, pred_co, pred_ne, pred_op = model(feats_input)\n",
    "#         pred_ex = model(feats_input)\n",
    "        \n",
    "        test_loss_ex = func(pred_ex, label_ex)\n",
    "        test_loss_ag = func(pred_ag, label_ag)\n",
    "        test_loss_co = func(pred_co, label_co)\n",
    "        test_loss_ne = func(pred_ne, label_ne)\n",
    "        test_loss_op = func(pred_op, label_op)\n",
    "\n",
    "        loss_list_test_ex.append(test_loss_ex.item())\n",
    "        loss_list_test_ag.append(test_loss_ag.item())\n",
    "        loss_list_test_co.append(test_loss_co.item())\n",
    "        loss_list_test_ne.append(test_loss_ne.item())\n",
    "        loss_list_test_op.append(test_loss_op.item())\n",
    "\n",
    "        for i in pred_ex:\n",
    "            pred_test_ex.append(i.detach().numpy())\n",
    "        for i in pred_ag:\n",
    "            pred_test_ag.append(i.detach().numpy())\n",
    "        for i in pred_co:\n",
    "            pred_test_co.append(i.detach().numpy())\n",
    "        for i in pred_ne:\n",
    "            pred_test_ne.append(i.detach().numpy())\n",
    "        for i in pred_op:\n",
    "            pred_test_op.append(i.detach().numpy())\n",
    "\n",
    "    # accuracy\n",
    "    label_train_ex = np.array(label_train_ex)\n",
    "    label_train_ag = np.array(label_train_ag)\n",
    "    label_train_co = np.array(label_train_co)\n",
    "    label_train_ne = np.array(label_train_ne)\n",
    "    label_train_op = np.array(label_train_op)\n",
    "    pred_train_ex = np.array(pred_train_ex)\n",
    "    pred_train_ag = np.array(pred_train_ag)\n",
    "    pred_train_co = np.array(pred_train_co)\n",
    "    pred_train_ne = np.array(pred_train_ne)\n",
    "    pred_train_op = np.array(pred_train_op)\n",
    "    pred_test_ex = np.array(pred_test_ex)\n",
    "    pred_test_ag = np.array(pred_test_ag)\n",
    "    pred_test_co = np.array(pred_test_co)\n",
    "    pred_test_ne = np.array(pred_test_ne)\n",
    "    pred_test_op = np.array(pred_test_op)\n",
    "\n",
    "    pred_train_ex = [np.argmax(p) for p in pred_train_ex]\n",
    "    pred_train_ag = [np.argmax(p) for p in pred_train_ag]\n",
    "    pred_train_co = [np.argmax(p) for p in pred_train_co]\n",
    "    pred_train_ne = [np.argmax(p) for p in pred_train_ne]\n",
    "    pred_train_op = [np.argmax(p) for p in pred_train_op]\n",
    "    pred_test_ex = [np.argmax(p) for p in pred_test_ex]\n",
    "    pred_test_ag = [np.argmax(p) for p in pred_test_ag]\n",
    "    pred_test_co = [np.argmax(p) for p in pred_test_co]\n",
    "    pred_test_ne = [np.argmax(p) for p in pred_test_ne]\n",
    "    pred_test_op = [np.argmax(p) for p in pred_test_op]\n",
    "\n",
    "#     print(np.rint(pred_train_ex))\n",
    "\n",
    "    train_acc_ex = recall_score(label_train_ex, pred_train_ex, average='macro')\n",
    "    train_acc_ag = recall_score(label_train_ag, np.rint(pred_train_ag), average='macro')\n",
    "    train_acc_co = recall_score(label_train_co, np.rint(pred_train_co), average='macro')\n",
    "    train_acc_ne = recall_score(label_train_ne, np.rint(pred_train_ne), average='macro')\n",
    "    train_acc_op = recall_score(label_train_op, np.rint(pred_train_op), average='macro')\n",
    "    test_acc_ex = recall_score(test_ex.numpy(), pred_test_ex, average='macro')\n",
    "    test_acc_ag = recall_score(test_ag.numpy(), np.rint(pred_test_ag), average='macro')\n",
    "    test_acc_co = recall_score(test_co.numpy(), np.rint(pred_test_co), average='macro')\n",
    "    test_acc_ne = recall_score(test_ne.numpy(), np.rint(pred_test_ne), average='macro')\n",
    "    test_acc_op = recall_score(test_op.numpy(), np.rint(pred_test_op), average='macro')\n",
    "\n",
    "    train_loss = (sum(loss_list_train_ex) / len(loss_list_train_ex)) \n",
    "    + (sum(loss_list_train_ag) / len(loss_list_train_ag))\n",
    "    + (sum(loss_list_train_co) / len(loss_list_train_co))\n",
    "    + (sum(loss_list_train_ne) / len(loss_list_train_ne))\n",
    "    + (sum(loss_list_train_op) / len(loss_list_train_op))\n",
    "    test_loss = (sum(loss_list_test_ex) / len(loss_list_test_ex))\n",
    "    + (sum(loss_list_test_ag) / len(loss_list_test_ag))\n",
    "    + (sum(loss_list_test_co) / len(loss_list_test_co))\n",
    "    + (sum(loss_list_test_ne) / len(loss_list_test_ne))\n",
    "    + (sum(loss_list_test_op) / len(loss_list_test_op))\n",
    "        \n",
    "    print('train_loss: %.4f' % train_loss, '|test_loss: %.4f' % test_loss, '\\n'\n",
    "          'train_acc_ex: %.4f' % train_acc_ex, '|train_acc_ag: %.4f' % train_acc_ag, '|train_acc_co: %.4f' % train_acc_co, '|train_acc_ne: %.4f' % train_acc_ne, '|train_acc_op: %.4f' % train_acc_op, '\\n'\n",
    "          'test_acc_ex: %.4f' % test_acc_ex, '|test_acc_ag: %.4f' % test_acc_ag, '|test_acc_co: %.4f' % test_acc_co, '|test_acc_ne: %.4f' % test_acc_ne, '|test_acc_op: %.4f' % test_acc_op)\n",
    "\n",
    "#     print('train_loss: %.4f' % train_loss, '|test_loss: %.4f' % test_loss, '\\n'\n",
    "#           'train_acc_ex: %.4f' % train_acc_ex, 'test_acc_ex: %.4f' % test_acc_ex)\n",
    "\n",
    "    stop = timeit.default_timer()\n",
    "    print('Time: ', stop - start)\n",
    "#     scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
