{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import torch\n",
    "import random\n",
    "import timeit\n",
    "import librosa\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from scipy import signal\n",
    "import torch.optim as optim\n",
    "from statistics import mode\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from transformers import Wav2Vec2ForCTC, Wav2Vec2Processor, Wav2Vec2Model\n",
    "from sklearn.metrics import classification_report, recall_score, accuracy_score, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at facebook/wav2vec2-large-xlsr-53-french were not used when initializing Wav2Vec2Model: ['lm_head.bias', 'lm_head.weight']\n",
      "- This IS expected if you are initializing Wav2Vec2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing Wav2Vec2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "processor = Wav2Vec2Processor.from_pretrained(\"facebook/wav2vec2-large-xlsr-53-french\")\n",
    "asr_model = Wav2Vec2Model.from_pretrained(\"facebook/wav2vec2-large-xlsr-53-french\").to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# recola label extraction\n",
    "\n",
    "aro_label_raw = []\n",
    "val_label_raw = []\n",
    "\n",
    "path = '/afs/inf.ed.ac.uk/user/s20/s2057508/Documents/Corpora/RECOLA/label/arousal/'\n",
    "os.chdir(path)\n",
    "for filename in ['P16','P17','P19','P21','P23','P25','P26','P28','P30','P34','P37','P39','P41','P42','P43','P45','P46','P48','P56','P58','P62','P64','P65']:\n",
    "    with open(filename + '.csv') as f:\n",
    "        file_content = csv.reader(f, delimiter=',')\n",
    "        headers = next(file_content, None)\n",
    "        for row in list(file_content)[1:]:\n",
    "            aro_label_raw.append(int(row[-1]))\n",
    "    f.close()\n",
    "            \n",
    "path = '/afs/inf.ed.ac.uk/user/s20/s2057508/Documents/Corpora/RECOLA/label/valence/'\n",
    "os.chdir(path)\n",
    "for filename in ['P16','P17','P19','P21','P23','P25','P26','P28','P30','P34','P37','P39','P41','P42','P43','P45','P46','P48','P56','P58','P62','P64','P65']:\n",
    "    with open(filename + '.csv') as f:\n",
    "        file_content = csv.reader(f, delimiter=',')\n",
    "        headers = next(file_content, None)\n",
    "        for row in list(file_content)[1:]:\n",
    "            val_label_raw.append(int(row[-1]))\n",
    "    f.close()\n",
    "    \n",
    "aro_label = []\n",
    "val_label = []\n",
    "\n",
    "# 10s\n",
    "for i in range(0,690):\n",
    "    if aro_label_raw[250*i:250*(i+1)].count(0) != aro_label_raw[250*i:250*(i+1)].count(1):\n",
    "        aro_label.append(mode(aro_label_raw[250*i:250*(i+1)]))\n",
    "    else:\n",
    "        aro_label.append(1)\n",
    "for i in range(0,690):\n",
    "    if val_label_raw[250*i:250*(i+1)].count(0) != val_label_raw[250*i:250*(i+1)].count(1):\n",
    "        val_label.append(mode(val_label_raw[250*i:250*(i+1)]))\n",
    "    else:\n",
    "        val_label.append(1)\n",
    "        \n",
    "# 5s\n",
    "# for i in range(0,1380):\n",
    "#     if aro_label_raw[125*i:125*(i+1)].count(0) != aro_label_raw[125*i:125*(i+1)].count(1):\n",
    "#         aro_label.append(mode(aro_label_raw[125*i:125*(i+1)]))\n",
    "#     else:\n",
    "#         aro_label.append(1)\n",
    "# for i in range(0,1380):\n",
    "#     if val_label_raw[125*i:125*(i+1)].count(0) != val_label_raw[125*i:125*(i+1)].count(1):\n",
    "#         val_label.append(mode(val_label_raw[125*i:125*(i+1)]))\n",
    "#     else:\n",
    "#         val_label.append(1)\n",
    "        \n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P16\n",
      "P17\n",
      "P19\n",
      "P21\n",
      "P23\n",
      "P25\n",
      "P26\n",
      "P28\n",
      "P30\n",
      "P34\n",
      "P37\n",
      "P39\n",
      "P41\n",
      "P42\n",
      "P43\n",
      "P45\n",
      "P46\n",
      "P48\n",
      "P56\n",
      "P58\n",
      "P62\n",
      "P64\n",
      "P65\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# recola feature extraction\n",
    "\n",
    "path = '/afs/inf.ed.ac.uk/user/s20/s2057508/Documents/Corpora/RECOLA/audio/'\n",
    "# path = '/afs/inf.ed.ac.uk/user/s20/s2057508/Documents/Corpora/RECOLA/audio2/'\n",
    "os.chdir(path)\n",
    "\n",
    "train_asr_hidden0 = []\n",
    "train_asr_hidden12 = []\n",
    "train_asr_hiddenave = []\n",
    "\n",
    "for dirname in ['P16','P17','P19','P21','P23','P25','P26','P28','P30','P34','P37','P39','P41','P42','P43','P45','P46','P48','P56','P58','P62','P64','P65']:\n",
    "    os.chdir(path + dirname)\n",
    "    print(dirname)\n",
    "    for filename in ['0.wav','1.wav','2.wav','3.wav','4.wav','5.wav','6.wav','7.wav','8.wav','9.wav','10.wav','11.wav','12.wav','13.wav','14.wav','15.wav','16.wav','17.wav','18.wav','19.wav','20.wav','21.wav','22.wav','23.wav','24.wav','25.wav','26.wav','27.wav','28.wav','29.wav']:\n",
    "#     for filename in ['0.wav','1.wav','2.wav','3.wav','4.wav','5.wav','6.wav','7.wav','8.wav','9.wav','10.wav','11.wav','12.wav','13.wav','14.wav','15.wav','16.wav','17.wav','18.wav','19.wav','20.wav','21.wav','22.wav','23.wav','24.wav','25.wav','26.wav','27.wav','28.wav','29.wav',\n",
    "#                     '30.wav','31.wav','32.wav','33.wav','34.wav','35.wav','36.wav','37.wav','38.wav','39.wav','40.wav','41.wav','42.wav','43.wav','44.wav','45.wav','46.wav','47.wav','48.wav','49.wav','50.wav','51.wav','52.wav','53.wav','54.wav','55.wav','56.wav','57.wav','58.wav','59.wav']:\n",
    "        audioinput, sr = librosa.load(filename, sr=16000)\n",
    "        torch.cuda.empty_cache()\n",
    "                \n",
    "        inputs = processor(audioinput, sampling_rate=sr, return_tensors='pt').input_values.to(DEVICE)\n",
    "        hiddens = asr_model(inputs, output_hidden_states=True).hidden_states\n",
    "        hidden0 = hiddens[0].detach()[0].mean(dim=0)\n",
    "        hidden1 = hiddens[1].detach()[0].mean(dim=0)\n",
    "        hidden2 = hiddens[2].detach()[0].mean(dim=0)\n",
    "        hidden3 = hiddens[3].detach()[0].mean(dim=0)\n",
    "        hidden4 = hiddens[4].detach()[0].mean(dim=0)\n",
    "        hidden5 = hiddens[5].detach()[0].mean(dim=0)\n",
    "        hidden6 = hiddens[6].detach()[0].mean(dim=0)\n",
    "        hidden7 = hiddens[7].detach()[0].mean(dim=0)\n",
    "        hidden8 = hiddens[8].detach()[0].mean(dim=0)\n",
    "        hidden9 = hiddens[9].detach()[0].mean(dim=0)\n",
    "        hidden10 = hiddens[10].detach()[0].mean(dim=0)\n",
    "        hidden11 = hiddens[11].detach()[0].mean(dim=0)\n",
    "        hidden12 = hiddens[12].detach()[0].mean(dim=0)\n",
    "        hidden13 = hiddens[13].detach()[0].mean(dim=0)\n",
    "        hidden14 = hiddens[14].detach()[0].mean(dim=0)\n",
    "        hidden15 = hiddens[15].detach()[0].mean(dim=0)\n",
    "        hidden16 = hiddens[16].detach()[0].mean(dim=0)\n",
    "        hidden17 = hiddens[17].detach()[0].mean(dim=0)\n",
    "        hidden18 = hiddens[18].detach()[0].mean(dim=0)\n",
    "        hidden19 = hiddens[19].detach()[0].mean(dim=0)\n",
    "        hidden20 = hiddens[20].detach()[0].mean(dim=0)\n",
    "        hidden21 = hiddens[21].detach()[0].mean(dim=0)\n",
    "        hidden22 = hiddens[22].detach()[0].mean(dim=0)\n",
    "        hidden23 = hiddens[23].detach()[0].mean(dim=0)\n",
    "        hidden24 = hiddens[24].detach()[0].mean(dim=0)\n",
    "\n",
    "        hidden = (hidden0 + hidden1 + hidden2 + hidden3 + hidden4 + hidden5 + hidden6 + hidden7 + hidden8 + hidden9 + hidden10 + hidden11\n",
    "                  + hidden12 + hidden13 + hidden14 + hidden15 + hidden16 + hidden17 + hidden18 + hidden19 + hidden20 + hidden21 + hidden22 + hidden23 + hidden24) / 25.0\n",
    "\n",
    "        train_asr_hidden0.append(hidden0.view(1, 1024).cpu().numpy())\n",
    "        train_asr_hidden12.append(hidden12.view(1, 1024).cpu().numpy())\n",
    "        train_asr_hiddenave.append(hidden.view(1, 1024).cpu().numpy())\n",
    "        \n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# sspnet feature & label preparation\n",
    "path = \"/afs/inf.ed.ac.uk/user/s20/s2057508/Documents/Corpora/SSPNet/Audio\"\n",
    "os.chdir(path)\n",
    "\n",
    "valid_ex = []\n",
    "valid_ag = []\n",
    "valid_co = []\n",
    "valid_ne = []\n",
    "valid_op = []\n",
    "test_ex = []\n",
    "test_ag = []\n",
    "test_co = []\n",
    "test_ne = []\n",
    "test_op = []\n",
    "\n",
    "valid_asr_hidden0 = []\n",
    "valid_asr_hidden12 = []\n",
    "valid_asr_hiddenave = []\n",
    "\n",
    "test_asr_hidden0 = []\n",
    "test_asr_hidden12 = []\n",
    "test_asr_hiddenave = []\n",
    "\n",
    "# valid feature\n",
    "with open('Scores.csv', 'r') as f:\n",
    "    file_content = csv.reader(f, delimiter=',')\n",
    "    headers = next(file_content, None)\n",
    "    for row in list(file_content)[:512]:\n",
    "        audioinput, sr = librosa.load(row[0] + '.wav', sr=16000)\n",
    "#         audioinput = audioinput[2:]\n",
    "#         if len(audioinput) < 160000:\n",
    "#             audioinput = np.pad(audioinput, (0, 160000-len(audioinput)), 'constant', constant_values=0)\n",
    "#             audioinput = torch.from_numpy(audioinput)\n",
    "        \n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "        inputs = processor(audioinput, sampling_rate=sr, return_tensors='pt').input_values.to(DEVICE)\n",
    "        hiddens = asr_model(inputs, output_hidden_states=True).hidden_states\n",
    "        hidden0 = hiddens[0].detach()[0].mean(dim=0)\n",
    "        hidden1 = hiddens[1].detach()[0].mean(dim=0)\n",
    "        hidden2 = hiddens[2].detach()[0].mean(dim=0)\n",
    "        hidden3 = hiddens[3].detach()[0].mean(dim=0)\n",
    "        hidden4 = hiddens[4].detach()[0].mean(dim=0)\n",
    "        hidden5 = hiddens[5].detach()[0].mean(dim=0)\n",
    "        hidden6 = hiddens[6].detach()[0].mean(dim=0)\n",
    "        hidden7 = hiddens[7].detach()[0].mean(dim=0)\n",
    "        hidden8 = hiddens[8].detach()[0].mean(dim=0)\n",
    "        hidden9 = hiddens[9].detach()[0].mean(dim=0)\n",
    "        hidden10 = hiddens[10].detach()[0].mean(dim=0)\n",
    "        hidden11 = hiddens[11].detach()[0].mean(dim=0)\n",
    "        hidden12 = hiddens[12].detach()[0].mean(dim=0)\n",
    "        hidden13 = hiddens[13].detach()[0].mean(dim=0)\n",
    "        hidden14 = hiddens[14].detach()[0].mean(dim=0)\n",
    "        hidden15 = hiddens[15].detach()[0].mean(dim=0)\n",
    "        hidden16 = hiddens[16].detach()[0].mean(dim=0)\n",
    "        hidden17 = hiddens[17].detach()[0].mean(dim=0)\n",
    "        hidden18 = hiddens[18].detach()[0].mean(dim=0)\n",
    "        hidden19 = hiddens[19].detach()[0].mean(dim=0)\n",
    "        hidden20 = hiddens[20].detach()[0].mean(dim=0)\n",
    "        hidden21 = hiddens[21].detach()[0].mean(dim=0)\n",
    "        hidden22 = hiddens[22].detach()[0].mean(dim=0)\n",
    "        hidden23 = hiddens[23].detach()[0].mean(dim=0)\n",
    "        hidden24 = hiddens[24].detach()[0].mean(dim=0)\n",
    "\n",
    "        hidden = (hidden0 + hidden1 + hidden2 + hidden3 + hidden4 + hidden5 + hidden6 + hidden7 + hidden8 + hidden9 + hidden10 + hidden11\n",
    "                  + hidden12 + hidden13 + hidden14 + hidden15 + hidden16 + hidden17 + hidden18 + hidden19 + hidden20 + hidden21 + hidden22 + hidden23 + hidden24) / 25.0\n",
    "\n",
    "        valid_asr_hidden0.append(hidden0.view(1, 1024).cpu().numpy())\n",
    "        valid_asr_hidden12.append(hidden6.view(1, 1024).cpu().numpy())\n",
    "        valid_asr_hiddenave.append(hidden.view(1, 1024).cpu().numpy())\n",
    "        \n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "        valid_ex.append(int(row[1]))\n",
    "        valid_ag.append(int(row[2]))\n",
    "        valid_co.append(int(row[3]))\n",
    "        valid_ne.append(int(row[4]))\n",
    "        valid_op.append(int(row[5]))\n",
    "        \n",
    "f.close()\n",
    "        \n",
    "# test feature\n",
    "with open('Scores.csv', 'r') as f:\n",
    "    file_content = csv.reader(f, delimiter=',')\n",
    "    headers = next(file_content, None)\n",
    "    for row in list(file_content)[512:]:\n",
    "        audioinput, sr = librosa.load(row[0] + '.wav', sr=16000)\n",
    "#         audioinput = audioinput[2:]\n",
    "#         if len(audioinput) < 160000:\n",
    "#             audioinput = np.pad(audioinput, (0, 160000-len(audioinput)), 'constant', constant_values=0)\n",
    "#             audioinput = torch.from_numpy(audioinput)\n",
    "        \n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "        inputs = processor(audioinput, sampling_rate=sr, return_tensors='pt').input_values.to(DEVICE)\n",
    "        hiddens = asr_model(inputs, output_hidden_states=True).hidden_states\n",
    "        hidden0 = hiddens[0].detach()[0].mean(dim=0)\n",
    "        hidden1 = hiddens[1].detach()[0].mean(dim=0)\n",
    "        hidden2 = hiddens[2].detach()[0].mean(dim=0)\n",
    "        hidden3 = hiddens[3].detach()[0].mean(dim=0)\n",
    "        hidden4 = hiddens[4].detach()[0].mean(dim=0)\n",
    "        hidden5 = hiddens[5].detach()[0].mean(dim=0)\n",
    "        hidden6 = hiddens[6].detach()[0].mean(dim=0)\n",
    "        hidden7 = hiddens[7].detach()[0].mean(dim=0)\n",
    "        hidden8 = hiddens[8].detach()[0].mean(dim=0)\n",
    "        hidden9 = hiddens[9].detach()[0].mean(dim=0)\n",
    "        hidden10 = hiddens[10].detach()[0].mean(dim=0)\n",
    "        hidden11 = hiddens[11].detach()[0].mean(dim=0)\n",
    "        hidden12 = hiddens[12].detach()[0].mean(dim=0)\n",
    "        hidden13 = hiddens[13].detach()[0].mean(dim=0)\n",
    "        hidden14 = hiddens[14].detach()[0].mean(dim=0)\n",
    "        hidden15 = hiddens[15].detach()[0].mean(dim=0)\n",
    "        hidden16 = hiddens[16].detach()[0].mean(dim=0)\n",
    "        hidden17 = hiddens[17].detach()[0].mean(dim=0)\n",
    "        hidden18 = hiddens[18].detach()[0].mean(dim=0)\n",
    "        hidden19 = hiddens[19].detach()[0].mean(dim=0)\n",
    "        hidden20 = hiddens[20].detach()[0].mean(dim=0)\n",
    "        hidden21 = hiddens[21].detach()[0].mean(dim=0)\n",
    "        hidden22 = hiddens[22].detach()[0].mean(dim=0)\n",
    "        hidden23 = hiddens[23].detach()[0].mean(dim=0)\n",
    "        hidden24 = hiddens[24].detach()[0].mean(dim=0)\n",
    "\n",
    "        hidden = (hidden0 + hidden1 + hidden2 + hidden3 + hidden4 + hidden5 + hidden6 + hidden7 + hidden8 + hidden9 + hidden10 + hidden11\n",
    "                  + hidden12 + hidden13 + hidden14 + hidden15 + hidden16 + hidden17 + hidden18 + hidden19 + hidden20 + hidden21 + hidden22 + hidden23 + hidden24) / 25.0\n",
    "\n",
    "        test_asr_hidden0.append(hidden0.view(1, 1024).cpu().numpy())\n",
    "        test_asr_hidden12.append(hidden6.view(1, 1024).cpu().numpy())\n",
    "        test_asr_hiddenave.append(hidden.view(1, 1024).cpu().numpy())\n",
    "        \n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "        test_ex.append(int(row[1]))\n",
    "        test_ag.append(int(row[2]))\n",
    "        test_co.append(int(row[3]))\n",
    "        test_ne.append(int(row[4]))\n",
    "        test_op.append(int(row[5]))\n",
    "\n",
    "f.close()\n",
    "\n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_asr_hidden0 = np.array(train_asr_hidden0)\n",
    "train_asr_hidden12 = np.array(train_asr_hidden12)\n",
    "train_asr_hiddenave = np.array(train_asr_hiddenave)\n",
    "train_asr_hidden0 = torch.from_numpy(train_asr_hidden0).to(DEVICE)\n",
    "train_asr_hidden12 = torch.from_numpy(train_asr_hidden12).to(DEVICE)\n",
    "train_asr_hiddenave = torch.from_numpy(train_asr_hiddenave).to(DEVICE)\n",
    "valid_asr_hidden0 = np.array(valid_asr_hidden0)\n",
    "valid_asr_hidden12 = np.array(valid_asr_hidden12)\n",
    "valid_asr_hiddenave = np.array(valid_asr_hiddenave)\n",
    "valid_asr_hidden0 = torch.from_numpy(valid_asr_hidden0).to(DEVICE)\n",
    "valid_asr_hidden12 = torch.from_numpy(valid_asr_hidden12).to(DEVICE)\n",
    "valid_asr_hiddenave = torch.from_numpy(valid_asr_hiddenave).to(DEVICE)\n",
    "test_asr_hidden0 = np.array(test_asr_hidden0)\n",
    "test_asr_hidden12 = np.array(test_asr_hidden12)\n",
    "test_asr_hiddenave = np.array(test_asr_hiddenave)\n",
    "test_asr_hidden0 = torch.from_numpy(test_asr_hidden0).to(DEVICE)\n",
    "test_asr_hidden12 = torch.from_numpy(test_asr_hidden12).to(DEVICE)\n",
    "test_asr_hiddenave = torch.from_numpy(test_asr_hiddenave).to(DEVICE)\n",
    "aro_label = torch.tensor(aro_label).to(DEVICE)\n",
    "val_label = torch.tensor(val_label).to(DEVICE)\n",
    "valid_ex = torch.tensor(valid_ex).to(DEVICE)\n",
    "valid_ag = torch.tensor(valid_ag).to(DEVICE)\n",
    "valid_co = torch.tensor(valid_co).to(DEVICE)\n",
    "valid_ne = torch.tensor(valid_ne).to(DEVICE)\n",
    "valid_op = torch.tensor(valid_op).to(DEVICE)\n",
    "test_ex = torch.tensor(test_ex).to(DEVICE)\n",
    "test_ag = torch.tensor(test_ag).to(DEVICE)\n",
    "test_co = torch.tensor(test_co).to(DEVICE)\n",
    "test_ne = torch.tensor(test_ne).to(DEVICE)\n",
    "test_op = torch.tensor(test_op).to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([690, 1, 1024]) torch.Size([512, 1, 1024]) torch.Size([128, 1, 1024])\n"
     ]
    }
   ],
   "source": [
    "print(train_asr_hidden0.size(), valid_asr_hidden0.size(), test_asr_hidden0.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset = TensorDataset(train_asr_hidden0, train_asr_hidden12, train_asr_hiddenave, aro_label, val_label)\n",
    "traindata = DataLoader(dataset=trainset, batch_size=64, shuffle=True)\n",
    "validset = TensorDataset(valid_asr_hidden0, valid_asr_hidden12, valid_asr_hiddenave, valid_ex, valid_ag, valid_co, valid_ne, valid_op)\n",
    "validdata = DataLoader(dataset=validset, batch_size=64, shuffle=True)\n",
    "testset = TensorDataset(test_asr_hidden0, test_asr_hidden12, test_asr_hiddenave, test_ex, test_ag, test_co, test_ne, test_op)\n",
    "testdata = DataLoader(dataset=testset, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MLP\n",
    "class NeuralNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNet,self).__init__()\n",
    "        self.flat = nn.Flatten()\n",
    "        self.dense1 = nn.Linear(1024, 128)\n",
    "        self.drop = nn.Dropout(0.5)\n",
    "        self.dense = nn.Linear(128, 16)\n",
    "        self.acti = nn.ReLU()\n",
    "        self.out1 = nn.Linear(16, 2)\n",
    "        self.out2 = nn.Linear(16, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flat(x)\n",
    "        x = self.dense1(x)\n",
    "        x = self.drop(x)\n",
    "        x = self.dense(x)\n",
    "        x = self.drop(x)\n",
    "        res = self.acti(x)\n",
    "        aro = self.out1(res)\n",
    "#         val = self.out2(res)\n",
    "        return aro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 0.5\n",
      "20 0.5\n",
      "30 0.5\n",
      "40 0.5\n",
      "50 0.5\n",
      "60 0.5\n",
      "70 0.5\n",
      "80 0.5\n",
      "90 0.5\n",
      "100 0.5\n"
     ]
    }
   ],
   "source": [
    "# train w2v training (fine-tuning)\n",
    "\n",
    "# training\n",
    "\n",
    "for k in [10,20,30,40,50,60,70,80,90,100]:\n",
    "    \n",
    "    torch.manual_seed(1)\n",
    "\n",
    "    model = NeuralNet().to(DEVICE)\n",
    "    params = model.parameters()\n",
    "    params = list(asr_model.parameters()) + list(model.parameters())\n",
    "#     optimizer = torch.optim.AdamW(params, lr=1e-4, weight_decay=1e-5)\n",
    "    optimizer = torch.optim.AdamW(filter(lambda p: p.requires_grad, params), lr=1e-4, weight_decay=1e-5)\n",
    "    func = nn.CrossEntropyLoss()\n",
    "\n",
    "    for epoch in range(k):\n",
    "        \n",
    "        model.train()\n",
    "        asr_model.train()\n",
    "        \n",
    "        unfreeze = ['classifier', 'pooler', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23']\n",
    "        for name, param in asr_model.named_parameters():\n",
    "            if any([ll in name for ll in unfreeze]):\n",
    "                param.requires_grad = True\n",
    "            else:\n",
    "                param.requires_grad = False\n",
    "        \n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        predictions_train = []\n",
    "        labels_train = []\n",
    "        loss_train = []\n",
    "\n",
    "        for feats2, feats3, feats, labels2, labels in traindata:\n",
    "\n",
    "    # loss\n",
    "            preds = model(feats)\n",
    "            train_loss = func(preds, labels)\n",
    "            loss_train.append(train_loss.detach().cpu().numpy())\n",
    "\n",
    "            for i in preds:\n",
    "                predictions_train.append(i.detach().cpu().numpy())\n",
    "\n",
    "            for i in labels:\n",
    "                labels_train.append(i.detach().cpu().numpy())\n",
    "\n",
    "    # backprop\n",
    "            optimizer.zero_grad()\n",
    "            torch.nn.utils.clip_grad_norm_(params, 1.0)\n",
    "            train_loss.backward()\n",
    "            optimizer.step()\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "    # compute performance for each epoch\n",
    "        predictions_train = np.array(predictions_train)\n",
    "        labels_train = np.array(labels_train)\n",
    "        predictions_train = [np.argmax(p) for p in predictions_train]\n",
    "        acc_train = recall_score(labels_train, predictions_train, average='macro')\n",
    "        loss_train = sum(loss_train) / len(loss_train)\n",
    "\n",
    "\n",
    "# fine-tuning\n",
    "\n",
    "    torch.manual_seed(1)\n",
    "\n",
    "    best = []\n",
    "\n",
    "    for epoch in range(100):\n",
    "        \n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        predictions_valid = []\n",
    "        predictions_test = []\n",
    "        labels_valid = []\n",
    "        labels_test = []\n",
    "        loss_valid = []\n",
    "        loss_test = []\n",
    "\n",
    "\n",
    "        for feats2, feats3, feats, labels2, labels3, labels4, labels5, labels in validdata:\n",
    "            preds = model(feats)\n",
    "            valid_loss = func(preds, labels)\n",
    "            loss_valid.append(valid_loss.detach().cpu().numpy())\n",
    "\n",
    "            for i in preds:\n",
    "                predictions_valid.append(i.detach().cpu().numpy())\n",
    "            for i in labels:\n",
    "                labels_valid.append(i.detach().cpu().numpy())\n",
    "\n",
    "        # backprop\n",
    "            optimizer.zero_grad()\n",
    "            torch.nn.utils.clip_grad_norm_(params, 1.0)\n",
    "            valid_loss.backward()\n",
    "            optimizer.step()\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "        # test\n",
    "        \n",
    "        model.eval()\n",
    "        asr_model.eval()\n",
    "        \n",
    "        for feats2, feats3, feats, labels2, labels3, labels4, labels5, labels in testdata:\n",
    "            preds = model(feats)\n",
    "            test_loss = func(preds, labels)\n",
    "            loss_test.append(test_loss.detach().cpu().numpy())\n",
    "\n",
    "            for i in preds:\n",
    "                predictions_test.append(i.detach().cpu().numpy())\n",
    "            for i in labels:\n",
    "                labels_test.append(i.detach().cpu().numpy())\n",
    "\n",
    "        # compute performance for each epoch\n",
    "        predictions_valid = np.array(predictions_valid)\n",
    "        predictions_test = np.array(predictions_test)\n",
    "        labels_valid = np.array(labels_valid)\n",
    "        labels_test = np.array(labels_test)\n",
    "        predictions_valid = [np.argmax(p) for p in predictions_valid]\n",
    "        predictions_test = [np.argmax(p) for p in predictions_test]\n",
    "\n",
    "    #     print(predictions_test)    \n",
    "\n",
    "        acc_valid = recall_score(labels_valid, predictions_valid, average='macro')\n",
    "        acc_test = recall_score(labels_test, predictions_test, average='macro')\n",
    "        loss_valid = sum(loss_valid) / len(loss_valid)\n",
    "        loss_test = sum(loss_test) / len(loss_test)\n",
    "\n",
    "        best.append(acc_test)\n",
    "\n",
    "\n",
    "    print(k, round(max(best), 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 0.671\n",
      "20 0.6602\n",
      "30 0.6541\n",
      "40 0.6614\n",
      "50 0.6577\n",
      "60 0.6541\n",
      "70 0.6349\n",
      "80 0.6613\n",
      "90 0.6408\n",
      "100 0.6347\n"
     ]
    }
   ],
   "source": [
    "# train\n",
    "\n",
    "# training\n",
    "\n",
    "for k in [10,20,30,40,50,60,70,80,90,100]:\n",
    "    \n",
    "    torch.manual_seed(1)\n",
    "\n",
    "    model = NeuralNet().to(DEVICE)\n",
    "    params = model.parameters()\n",
    "    optimizer = torch.optim.AdamW(params, lr=1e-4, weight_decay=1e-5)\n",
    "    func = nn.CrossEntropyLoss()\n",
    "\n",
    "    for epoch in range(k):\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        predictions_train = []\n",
    "        labels_train = []\n",
    "        loss_train = []\n",
    "\n",
    "        for feats2, feats, feats3, labels, labels2 in traindata:\n",
    "\n",
    "    # loss\n",
    "            preds = model(feats)\n",
    "            train_loss = func(preds, labels)\n",
    "            loss_train.append(train_loss.detach().cpu().numpy())\n",
    "\n",
    "            for i in preds:\n",
    "                predictions_train.append(i.detach().cpu().numpy())\n",
    "\n",
    "            for i in labels:\n",
    "                labels_train.append(i.detach().cpu().numpy())\n",
    "\n",
    "    # backprop\n",
    "            optimizer.zero_grad()\n",
    "            torch.nn.utils.clip_grad_norm_(params, 1.0)\n",
    "            train_loss.backward()\n",
    "            optimizer.step()\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "    # compute performance for each epoch\n",
    "        predictions_train = np.array(predictions_train)\n",
    "        labels_train = np.array(labels_train)\n",
    "        predictions_train = [np.argmax(p) for p in predictions_train]\n",
    "        acc_train = recall_score(labels_train, predictions_train, average='macro')\n",
    "        loss_train = sum(loss_train) / len(loss_train)\n",
    "\n",
    "\n",
    "# fine-tuning\n",
    "\n",
    "    torch.manual_seed(1)\n",
    "\n",
    "    best = []\n",
    "    \n",
    "#     model.eval()\n",
    "\n",
    "    for epoch in range(100):\n",
    "        # valid\n",
    "        \n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        predictions_valid = []\n",
    "        predictions_test = []\n",
    "        labels_valid = []\n",
    "        labels_test = []\n",
    "        loss_valid = []\n",
    "        loss_test = []\n",
    "\n",
    "\n",
    "        for feats2, feats, feats3, labels, labels2, labels3, labels4, labels5 in validdata:\n",
    "            preds = model(feats)\n",
    "            valid_loss = func(preds, labels)\n",
    "            loss_valid.append(valid_loss.detach().cpu().numpy())\n",
    "\n",
    "            for i in preds:\n",
    "                predictions_valid.append(i.detach().cpu().numpy())\n",
    "            for i in labels:\n",
    "                labels_valid.append(i.detach().cpu().numpy())\n",
    "\n",
    "        # backprop\n",
    "            optimizer.zero_grad()\n",
    "            torch.nn.utils.clip_grad_norm_(params, 1.0)\n",
    "            valid_loss.backward()\n",
    "            optimizer.step()\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "        # test\n",
    "        \n",
    "        model.eval()\n",
    "        for feats2, feats, feats3, labels, labels2, labels3, labels4, labels5 in testdata:\n",
    "            preds = model(feats)\n",
    "            test_loss = func(preds, labels)\n",
    "            loss_test.append(test_loss.detach().cpu().numpy())\n",
    "\n",
    "            for i in preds:\n",
    "                predictions_test.append(i.detach().cpu().numpy())\n",
    "            for i in labels:\n",
    "                labels_test.append(i.detach().cpu().numpy())\n",
    "\n",
    "        # compute performance for each epoch\n",
    "        predictions_valid = np.array(predictions_valid)\n",
    "        predictions_test = np.array(predictions_test)\n",
    "        labels_valid = np.array(labels_valid)\n",
    "        labels_test = np.array(labels_test)\n",
    "        predictions_valid = [np.argmax(p) for p in predictions_valid]\n",
    "        predictions_test = [np.argmax(p) for p in predictions_test]\n",
    "\n",
    "    #     print(predictions_test)    \n",
    "\n",
    "        acc_valid = recall_score(labels_valid, predictions_valid, average='macro')\n",
    "        acc_test = recall_score(labels_test, predictions_test, average='macro')\n",
    "        loss_valid = sum(loss_valid) / len(loss_valid)\n",
    "        loss_test = sum(loss_test) / len(loss_test)\n",
    "\n",
    "        best.append(acc_test)\n",
    "\n",
    "\n",
    "    print(k, round(max(best), 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 0.5733 0.5\n",
      "20 0.6503 0.5\n",
      "30 0.5075 0.5365\n",
      "40 0.6371 0.5\n",
      "50 0.5122 0.5565\n",
      "60 0.6382 0.558\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-cc5b2ac4ffe9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    109\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip_grad_norm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m             \u001b[0mvalid_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty_cache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     86\u001b[0m                 \u001b[0mprofile_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Optimizer.step#{}.step\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecord_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprofile_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/autograd/grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/optim/adamw.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    146\u001b[0m                     \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'lr'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m                     \u001b[0mweight_decay\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'weight_decay'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 148\u001b[0;31m                     eps=group['eps'])\n\u001b[0m\u001b[1;32m    149\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    150\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/optim/_functional.py\u001b[0m in \u001b[0;36madamw\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, amsgrad, beta1, beta2, lr, weight_decay, eps)\u001b[0m\n\u001b[1;32m    137\u001b[0m             \u001b[0mdenom\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmax_exp_avg_sqs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias_correction2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m             \u001b[0mdenom\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mexp_avg_sq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias_correction2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m         \u001b[0mstep_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlr\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mbias_correction1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# train-multitask w2v training (fine-tuning)\n",
    "\n",
    "# training\n",
    "\n",
    "for k in [10,20,30,40,50,60,70,80,90,100]:\n",
    "    \n",
    "    torch.manual_seed(1)\n",
    "\n",
    "    model = NeuralNet().to(DEVICE)\n",
    "    params = model.parameters()\n",
    "    optimizer = torch.optim.AdamW(params, lr=1e-4, weight_decay=1e-5)\n",
    "    func = nn.CrossEntropyLoss()\n",
    "\n",
    "    for epoch in range(k):\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        model.train()\n",
    "        asr_model.train()\n",
    "\n",
    "        predictions1_train = []\n",
    "        predictions2_train = []\n",
    "        labels1_train = []\n",
    "        labels2_train = []\n",
    "        loss_train = []\n",
    "\n",
    "        for feats1, feats2, feats3, labels1, labels2 in traindata:\n",
    "\n",
    "    # loss\n",
    "            preds1, preds2 = model(feats1)\n",
    "            train_loss1 = func(preds1, labels1)\n",
    "            train_loss2 = func(preds2, labels2)\n",
    "            train_loss = train_loss1 + train_loss2\n",
    "            loss_train.append(train_loss.detach().cpu().numpy())\n",
    "\n",
    "            for i in preds1:\n",
    "                predictions1_train.append(i.detach().cpu().numpy())                \n",
    "            for i in preds2:\n",
    "                predictions2_train.append(i.detach().cpu().numpy())\n",
    "\n",
    "            for i in labels1:\n",
    "                labels1_train.append(i.detach().cpu().numpy())\n",
    "            for i in labels2:\n",
    "                labels2_train.append(i.detach().cpu().numpy())\n",
    "                \n",
    "    # backprop\n",
    "            optimizer.zero_grad()\n",
    "            torch.nn.utils.clip_grad_norm_(params, 1.0)\n",
    "            train_loss.backward()\n",
    "            optimizer.step()\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "    # compute performance for each epoch\n",
    "        predictions1_train = np.array(predictions1_train)\n",
    "        predictions2_train = np.array(predictions2_train)\n",
    "        labels1_train = np.array(labels1_train)\n",
    "        labels2_train = np.array(labels2_train)\n",
    "        predictions1_train = [np.argmax(p) for p in predictions1_train]\n",
    "        predictions2_train = [np.argmax(p) for p in predictions2_train]\n",
    "        acc1_train = recall_score(labels1_train, predictions1_train, average='macro')\n",
    "        acc2_train = recall_score(labels2_train, predictions2_train, average='macro')\n",
    "        loss_train = sum(loss_train) / len(loss_train)\n",
    "\n",
    "\n",
    "# fine-tuning\n",
    "\n",
    "#     torch.manual_seed(1)\n",
    "\n",
    "    best1 = []\n",
    "    best2 = []\n",
    "    \n",
    "    model.eval()\n",
    "    asr_model.eval()\n",
    "\n",
    "    for epoch in range(100):\n",
    "        # valid\n",
    "        \n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        predictions1_valid = []\n",
    "        predictions2_valid = []\n",
    "        predictions1_test = []\n",
    "        predictions2_test = []\n",
    "        labels1_valid = []\n",
    "        labels2_valid = []\n",
    "        labels1_test = []\n",
    "        labels2_test = []\n",
    "        loss_valid = []\n",
    "        loss_test = []\n",
    "\n",
    "\n",
    "        for feats1, feats2, feats3, labels1, labels3, labels4, labels5, labels2 in validdata:\n",
    "            preds1, preds2 = model(feats1)\n",
    "            valid_loss1 = func(preds1, labels1)\n",
    "            valid_loss2 = func(preds2, labels2)\n",
    "            valid_loss = valid_loss1 + valid_loss2\n",
    "            loss_valid.append(valid_loss.detach().cpu().numpy())\n",
    "\n",
    "            for i in preds1:\n",
    "                predictions1_valid.append(i.detach().cpu().numpy())\n",
    "            for i in preds2:\n",
    "                predictions2_valid.append(i.detach().cpu().numpy())\n",
    "            for i in labels1:\n",
    "                labels1_valid.append(i.detach().cpu().numpy())\n",
    "            for i in labels2:\n",
    "                labels2_valid.append(i.detach().cpu().numpy())\n",
    "                \n",
    "        # backprop\n",
    "            optimizer.zero_grad()\n",
    "            torch.nn.utils.clip_grad_norm_(params, 1.0)\n",
    "            valid_loss.backward()\n",
    "            optimizer.step()\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "        # test\n",
    "        \n",
    "        for feats1, feats2, feats3, labels1, labels3, labels4, labels5, labels2 in testdata:\n",
    "            preds1, preds2 = model(feats1)\n",
    "            test_loss1 = func(preds1, labels1)\n",
    "            test_loss2 = func(preds2, labels2)\n",
    "            test_loss = test_loss1 + test_loss2\n",
    "            loss_test.append(test_loss.detach().cpu().numpy())\n",
    "\n",
    "            for i in preds1:\n",
    "                predictions1_test.append(i.detach().cpu().numpy())\n",
    "            for i in preds2:\n",
    "                predictions2_test.append(i.detach().cpu().numpy())\n",
    "            for i in labels1:\n",
    "                labels1_test.append(i.detach().cpu().numpy())\n",
    "            for i in labels2:\n",
    "                labels2_test.append(i.detach().cpu().numpy())\n",
    "\n",
    "        # compute performance for each epoch\n",
    "        predictions1_valid = np.array(predictions1_valid)\n",
    "        predictions2_valid = np.array(predictions2_valid)\n",
    "        predictions1_test = np.array(predictions1_test)\n",
    "        predictions2_test = np.array(predictions2_test)\n",
    "        labels1_valid = np.array(labels1_valid)\n",
    "        labels2_valid = np.array(labels2_valid)\n",
    "        labels1_test = np.array(labels1_test)\n",
    "        labels2_test = np.array(labels2_test)\n",
    "        predictions1_valid = [np.argmax(p) for p in predictions1_valid]\n",
    "        predictions2_valid = [np.argmax(p) for p in predictions2_valid]\n",
    "        predictions1_test = [np.argmax(p) for p in predictions1_test]\n",
    "        predictions2_test = [np.argmax(p) for p in predictions2_test]\n",
    "\n",
    "    #     print(predictions_test)    \n",
    "\n",
    "        acc1_valid = recall_score(labels1_valid, predictions1_valid, average='macro')\n",
    "        acc2_valid = recall_score(labels2_valid, predictions2_valid, average='macro')\n",
    "        acc1_test = recall_score(labels1_test, predictions1_test, average='macro')\n",
    "        acc2_test = recall_score(labels2_test, predictions2_test, average='macro')\n",
    "        loss_valid = sum(loss_valid) / len(loss_valid)\n",
    "        loss_test = sum(loss_test) / len(loss_test)\n",
    "\n",
    "        best1.append(acc1_test)\n",
    "        best2.append(acc2_test)\n",
    "\n",
    "    print(k, round(max(best1), 4), round(max(best2), 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train-multitask\n",
    "\n",
    "# training\n",
    "\n",
    "for k in [10,20,30,40,50,60,70,80,90,100]:\n",
    "    \n",
    "#     torch.manual_seed(1)\n",
    "\n",
    "    model = NeuralNet().to(DEVICE)\n",
    "    params = model.parameters()\n",
    "    optimizer = torch.optim.AdamW(params, lr=1e-4, weight_decay=1e-5)\n",
    "    func = nn.CrossEntropyLoss()\n",
    "\n",
    "    for epoch in range(k):\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        predictions1_train = []\n",
    "        predictions2_train = []\n",
    "        labels1_train = []\n",
    "        labels2_train = []\n",
    "        loss_train = []\n",
    "\n",
    "        for feats1, feats2, feats3, labels1, labels2 in traindata:\n",
    "\n",
    "    # loss\n",
    "            preds1, preds2 = model(feats1)\n",
    "            train_loss1 = func(preds1, labels1)\n",
    "            train_loss2 = func(preds2, labels2)\n",
    "            train_loss = train_loss1 + train_loss2\n",
    "            loss_train.append(train_loss.detach().cpu().numpy())\n",
    "\n",
    "            for i in preds1:\n",
    "                predictions1_train.append(i.detach().cpu().numpy())                \n",
    "            for i in preds2:\n",
    "                predictions2_train.append(i.detach().cpu().numpy())\n",
    "\n",
    "            for i in labels1:\n",
    "                labels1_train.append(i.detach().cpu().numpy())\n",
    "            for i in labels2:\n",
    "                labels2_train.append(i.detach().cpu().numpy())\n",
    "                \n",
    "    # backprop\n",
    "            optimizer.zero_grad()\n",
    "            torch.nn.utils.clip_grad_norm_(params, 1.0)\n",
    "            train_loss.backward()\n",
    "            optimizer.step()\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "    # compute performance for each epoch\n",
    "        predictions1_train = np.array(predictions1_train)\n",
    "        predictions2_train = np.array(predictions2_train)\n",
    "        labels1_train = np.array(labels1_train)\n",
    "        labels2_train = np.array(labels2_train)\n",
    "        predictions1_train = [np.argmax(p) for p in predictions1_train]\n",
    "        predictions2_train = [np.argmax(p) for p in predictions2_train]\n",
    "        acc1_train = recall_score(labels1_train, predictions1_train, average='macro')\n",
    "        acc2_train = recall_score(labels2_train, predictions2_train, average='macro')\n",
    "        loss_train = sum(loss_train) / len(loss_train)\n",
    "\n",
    "\n",
    "# fine-tuning\n",
    "\n",
    "#     torch.manual_seed(1)\n",
    "\n",
    "    best1 = []\n",
    "    best2 = []\n",
    "    \n",
    "    model.eval()\n",
    "\n",
    "    for epoch in range(100):\n",
    "        # valid\n",
    "        \n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        predictions1_valid = []\n",
    "        predictions2_valid = []\n",
    "        predictions1_test = []\n",
    "        predictions2_test = []\n",
    "        labels1_valid = []\n",
    "        labels2_valid = []\n",
    "        labels1_test = []\n",
    "        labels2_test = []\n",
    "        loss_valid = []\n",
    "        loss_test = []\n",
    "\n",
    "\n",
    "        for feats1, feats2, feats3, labels1, labels3, labels4, labels5, labels2 in validdata:\n",
    "            preds1, preds2 = model(feats1)\n",
    "            valid_loss1 = func(preds1, labels1)\n",
    "            valid_loss2 = func(preds2, labels2)\n",
    "            valid_loss = valid_loss1 + valid_loss2\n",
    "            loss_valid.append(valid_loss.detach().cpu().numpy())\n",
    "\n",
    "            for i in preds1:\n",
    "                predictions1_valid.append(i.detach().cpu().numpy())\n",
    "            for i in preds2:\n",
    "                predictions2_valid.append(i.detach().cpu().numpy())\n",
    "            for i in labels1:\n",
    "                labels1_valid.append(i.detach().cpu().numpy())\n",
    "            for i in labels2:\n",
    "                labels2_valid.append(i.detach().cpu().numpy())\n",
    "                \n",
    "        # backprop\n",
    "            optimizer.zero_grad()\n",
    "            torch.nn.utils.clip_grad_norm_(params, 1.0)\n",
    "            valid_loss.backward()\n",
    "            optimizer.step()\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "        # test\n",
    "        \n",
    "        for feats1, feats2, feats3, labels1, labels3, labels4, labels5, labels2 in testdata:\n",
    "            preds1, preds2 = model(feats1)\n",
    "            test_loss1 = func(preds1, labels1)\n",
    "            test_loss2 = func(preds2, labels2)\n",
    "            test_loss = test_loss1 + test_loss2\n",
    "            loss_test.append(test_loss.detach().cpu().numpy())\n",
    "\n",
    "            for i in preds1:\n",
    "                predictions1_test.append(i.detach().cpu().numpy())\n",
    "            for i in preds2:\n",
    "                predictions2_test.append(i.detach().cpu().numpy())\n",
    "            for i in labels1:\n",
    "                labels1_test.append(i.detach().cpu().numpy())\n",
    "            for i in labels2:\n",
    "                labels2_test.append(i.detach().cpu().numpy())\n",
    "\n",
    "        # compute performance for each epoch\n",
    "        predictions1_valid = np.array(predictions1_valid)\n",
    "        predictions2_valid = np.array(predictions2_valid)\n",
    "        predictions1_test = np.array(predictions1_test)\n",
    "        predictions2_test = np.array(predictions2_test)\n",
    "        labels1_valid = np.array(labels1_valid)\n",
    "        labels2_valid = np.array(labels2_valid)\n",
    "        labels1_test = np.array(labels1_test)\n",
    "        labels2_test = np.array(labels2_test)\n",
    "        predictions1_valid = [np.argmax(p) for p in predictions1_valid]\n",
    "        predictions2_valid = [np.argmax(p) for p in predictions2_valid]\n",
    "        predictions1_test = [np.argmax(p) for p in predictions1_test]\n",
    "        predictions2_test = [np.argmax(p) for p in predictions2_test]\n",
    "\n",
    "    #     print(predictions_test)    \n",
    "\n",
    "        acc1_valid = recall_score(labels1_valid, predictions1_valid, average='macro')\n",
    "        acc2_valid = recall_score(labels2_valid, predictions2_valid, average='macro')\n",
    "        acc1_test = recall_score(labels1_test, predictions1_test, average='macro')\n",
    "        acc2_test = recall_score(labels2_test, predictions2_test, average='macro')\n",
    "        loss_valid = sum(loss_valid) / len(loss_valid)\n",
    "        loss_test = sum(loss_test) / len(loss_test)\n",
    "\n",
    "        best1.append(acc1_test)\n",
    "        best2.append(acc2_test)\n",
    "\n",
    "    print(k, round(max(best1), 4), round(max(best2), 4))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
